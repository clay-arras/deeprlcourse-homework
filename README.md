Assignments for [Berkeley CS 285: Deep Reinforcement Learning, Decision Making, and Control](http://rail.eecs.berkeley.edu/deeprlcourse/).


Week 1, Jan 21 – Jan 27:
- Lecture 1: Introduction and Course Overview
- Read Chapter 4.1–4.3: Policy Evaluation (Prediction), Policy Improvement, Policy Iteration
Week 2, Jan 28 – Feb 3:
- Lecture 2: Supervised Learning of Behaviors
- Read Chapter 4.4–4.6: Value Iteration, Asynchronous Dynamic Programming, Generalized Policy Iteration
Week 3, Feb 4 – Feb 10:
- Lecture 3: PyTorch Tutorial
- Review and summarize Chapter 4.7–4.8: Efficiency of Dynamic Programming, Summary
Week 4, Feb 11 – Feb 17:
- Homework 1: Imitation Learning (Complete)
- Begin Chapter 5.1–5.3: Monte Carlo Prediction, Monte Carlo Estimation of Action Values, Monte Carlo Control
Week 5, Feb 18 – Feb 24:
- Lecture 4: Introduction to Reinforcement Learning
- Continue Chapter 5.4–5.5: Monte Carlo Control without Exploring Starts, Off-policy Prediction via Importance Sampling
Week 6, Feb 25 – Mar 2:
- Lecture 5: Policy Gradients
- Review and summarize Chapter 5.6–5.8: Incremental Implementation, Off-policy Monte Carlo Control, Discounting-aware Importance Sampling
Week 7, Mar 3 – Mar 9:
- Lecture 6: Actor-Critic Algorithms
- Complete Chapter 6.1–6.3: TD Prediction, Advantages of TD Prediction Methods, Optimality of TD(0)
Week 8, Mar 10 – Mar 16:
- Homework 2: Policy Gradients (Complete)
- Continue Chapter 6.4–6.6: Sarsa, Q-learning, Expected Sarsa
Week 9, Mar 17 – Mar 23:
- Lecture 7: Value Function Methods
- Review and summarize Chapter 6.7–6.9: Maximization Bias, Games, Summary
Week 10, Mar 24 – Mar 30:
- Lecture 8: Deep RL with Q-Functions
- Begin Chapter 7.1–7.3: nn-step TD Prediction, nn-step Sarsa, nn-step Off-policy Learning
Week 11, Mar 31 – Apr 6:
- Homework 3: Q-learning and Actor-Critic Algorithms (Complete)
- Continue Chapter 7.4–7.6: Per-decision Methods, Tree Backup Algorithm, nn-step Q(λ)
Week 12, Apr 7 – Apr 13:
- Lecture 9: Advanced Policy Gradients
- Read Chapter 8.1–8.3: Models and Planning, Dyna, When the Model Is Wrong
Week 13, Apr 14 – Apr 20:
- Lecture 10: Optimal Control and Planning
- Continue Chapter 8.4–8.6: Prioritized Sweeping, Expected vs. Sample Updates, Trajectory Sampling
Week 14, Apr 21 – Apr 27:
- Homework 4: Model-Based Reinforcement Learning (Complete)
- Review and summarize Chapter 8.7–8.9: Real-time Dynamic Programming, Planning at Decision Time, Heuristic Search
